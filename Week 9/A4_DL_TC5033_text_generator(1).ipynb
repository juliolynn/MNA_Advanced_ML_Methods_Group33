{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Maestría en Inteligencia Artificial Aplicada**\n",
    "##**Advanced Machine Learning Methods (Gpo 10)**\n",
    "###Tecnológico de Monterrey\n",
    "###Profesor Ph.D. José Antonio Cantoral Ceballos\n",
    "\n",
    "## **Activity 4**\n",
    "\n",
    "### Text Generator\n",
    "\n",
    "##**Team 33**:\n",
    "\n",
    "### Humberto Lozano Cedillo A01363184\n",
    "### Julio Cesar Lynn Jimenez A01793660\n",
    "### Sarah Mendoza Medina A01215352\n",
    "### David Mireles Samaniego A01302935"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e89c8",
   "metadata": {},
   "source": [
    "## TC 5033\n",
    "### Text Generation\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
    "<br>\n",
    "\n",
    "- Objective:\n",
    "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
    "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
    "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Instructions:\n",
    "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
    "\n",
    "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
    "\n",
    "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation. \n",
    "\n",
    "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
    "\n",
    "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
    "\n",
    "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Evaluation Criteria:\n",
    "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
    "\n",
    "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
    "\n",
    "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function. \n",
    "\n",
    "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3eb4b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#PyTorch libraries\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import WikiText2\n",
    "# Dataloader library\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "# Libraries to prepare the data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# neural layers\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6d8ff971",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f3288ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = WikiText2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc4c7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = get_tokenizer('basic_english')\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokeniser(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2c2cb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "#set unknown token at position 0\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "134b832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "def data_process(raw_text_iter, seq_length = 50):\n",
    "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #remove empty tensors\n",
    "#     target_data = torch.cat(d)\n",
    "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length), \n",
    "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))  \n",
    "\n",
    "# # Create tensors for the training set\n",
    "x_train, y_train = data_process(train_dataset, seq_length)\n",
    "x_val, y_val = data_process(val_dataset, seq_length)\n",
    "x_test, y_test = data_process(test_dataset, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4b54c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f4d400fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # choose a batch size that fits your computation resources\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "59c63b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "# Feel free to experiment\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, text, hidden):\n",
    "        embeddings = self.embeddings(text)\n",
    "        output, hidden = self.lstm(embeddings, hidden)\n",
    "        decoded = self.fc(output)\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = len(vocab) # vocabulary size\n",
    "emb_size = 300 # embedding size\n",
    "neurons = 128 # the dimension of the feedforward network model, i.e. # of neurons \n",
    "num_layers = 2 # the number of nn.LSTM layers\n",
    "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, loader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (data, targets) in enumerate(loader):\n",
    "            # Detach the hidden state between batches\n",
    "            hidden = model.init_hidden(batch_size) \n",
    "            \n",
    "            xi = data.to(device=device, dtype = torch.int32)\n",
    "            yi = targets.to(device=device, dtype = torch.long) \n",
    "            yi_flatten = yi.view(-1)\n",
    "\n",
    "            y_pred, hidden = model(xi, hidden)\n",
    "            y_pred_max_prob, y_pred_max_idx = y_pred.max(dim=2)\n",
    "\n",
    "            total_acc += (y_pred_max_idx.view(-1) == yi_flatten).sum().item()\n",
    "            total_count += yi_flatten.size(0)\n",
    "            \n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.text import Perplexity\n",
    "\n",
    "def perplexity(model, loader):\n",
    "    model.eval()\n",
    "    perplexity_metric = Perplexity(device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, targets) in enumerate(loader):\n",
    "            # Detach the hidden state between batches\n",
    "            hidden = model.init_hidden(batch_size) \n",
    "            \n",
    "            xi = data.to(device=device, dtype = torch.int32)\n",
    "            yi = targets.to(device=device, dtype = torch.long) \n",
    "\n",
    "            y_pred, hidden = model(xi, hidden)\n",
    "\n",
    "            perplexity_metric.update(y_pred, yi)\n",
    "            \n",
    "    return perplexity_metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "215eabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimiser, loss_function, train_loader, val_loader):\n",
    "    '''\n",
    "    The following are possible instructions you may want to conside for this function.\n",
    "    This is only a guide and you may change add or remove whatever you consider appropriate\n",
    "    as long as you train your model correctly.\n",
    "        - loop through specified epochs\n",
    "        - loop through dataloader\n",
    "        - don't forget to zero grad!\n",
    "        - place data (both input and target) in device\n",
    "        - init hidden states e.g. hidden = model.init_hidden(batch_size)\n",
    "        - run the model\n",
    "        - compute the cost or loss\n",
    "        - backpropagation\n",
    "        - Update paratemers\n",
    "        - Include print all the information you consider helpful\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        total_acc, total_count = 0, 0\n",
    "        log_interval = int(len(train_loader) / 5)\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "\n",
    "        for i, (data, targets) in enumerate(train_loader):\n",
    "            # Detach the hidden state between batches\n",
    "            hidden = model.init_hidden(batch_size) \n",
    "            \n",
    "            # data_words = []\n",
    "            # for i in range(data.size(0)):\n",
    "            #     for j in range(data.size(1)):\n",
    "            #         idx = data[i][j]\n",
    "            #         data_words.append(vocab.lookup_token(idx))\n",
    "            #     data_words.append('|'*4)\n",
    "            # data_str = ' '.join(data_words)\n",
    "            # print(data_str)\n",
    "            \n",
    "\n",
    "            # target_words = []\n",
    "            # for i in range(targets.size(0)):\n",
    "            #     for j in range(targets.size(1)):\n",
    "            #         idx = targets[i][j]\n",
    "            #         target_words.append(vocab.lookup_token(idx))\n",
    "            #     target_words.append('|'*4)\n",
    "            # target_str = ' '.join(target_words)\n",
    "            # print('-'*40)\n",
    "            # print(target_str)\n",
    "        \n",
    "            xi = data.to(device=device, dtype = torch.int32)\n",
    "            yi = targets.to(device=device, dtype = torch.long) # Needs to be long for the loss_function to treat the target as class indices\n",
    "            yi_flatten = yi.view(-1)\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            y_pred, hidden = model(xi, hidden)\n",
    "            y_pred_max_prob, y_pred_max_idx = y_pred.max(dim=2)\n",
    "\n",
    "            # input needs to match the yi flattened size in one dimension and the vocab_size (our classes) on another\n",
    "            cost = loss_function(input=y_pred.view(batch_size*seq_length, vocab_size), target=yi_flatten)\n",
    "            cost.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimiser.step()\n",
    "\n",
    "            total_acc += (y_pred_max_idx.view(-1) == yi_flatten).sum().item()\n",
    "            total_count += yi_flatten.size(0)\n",
    "\n",
    "            perplexity_metric = Perplexity(device=device)\n",
    "            perplexity_metric.update(y_pred, yi)\n",
    "            perplex_test = perplexity_metric.compute()\n",
    "            \n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                print(\n",
    "                    \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                    \"| accuracy {:8.3f} | perplexity {:8.3f}\".format(\n",
    "                        epoch, i, len(train_loader), total_acc / total_count, perplex_test\n",
    "                    )\n",
    "                )\n",
    "            total_acc, total_count = 0, 0\n",
    "\n",
    "        accu_val = accuracy(model, val_loader)\n",
    "        perplex_val = perplexity(model, val_loader)\n",
    "\n",
    "        print(\"-\" * 59)\n",
    "        print(\n",
    "            \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "            \"valid accuracy {:8.3f} | valid perplexity {:8.3f} \".format(\n",
    "                epoch, time.time() - start_time, accu_val, perplex_val\n",
    "            )\n",
    "        )\n",
    "        print(\"-\" * 59)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "aa9c84ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    64/  320 batches | accuracy    0.065 | perplexity 1277.424\n",
      "| epoch   0 |   128/  320 batches | accuracy    0.065 | perplexity 1095.540\n",
      "| epoch   0 |   192/  320 batches | accuracy    0.065 | perplexity 1077.024\n",
      "| epoch   0 |   256/  320 batches | accuracy    0.064 | perplexity 1088.238\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   0 | time: 58.78s | valid accuracy    0.069 | valid perplexity  806.268 \n",
      "-----------------------------------------------------------\n",
      "| epoch   1 |    64/  320 batches | accuracy    0.066 | perplexity  977.097\n",
      "| epoch   1 |   128/  320 batches | accuracy    0.068 | perplexity  931.063\n",
      "| epoch   1 |   192/  320 batches | accuracy    0.083 | perplexity  908.169\n",
      "| epoch   1 |   256/  320 batches | accuracy    0.121 | perplexity  849.101\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 58.47s | valid accuracy    0.118 | valid perplexity  698.133 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |    64/  320 batches | accuracy    0.116 | perplexity  866.487\n",
      "| epoch   2 |   128/  320 batches | accuracy    0.118 | perplexity  803.457\n",
      "| epoch   2 |   192/  320 batches | accuracy    0.115 | perplexity  811.329\n",
      "| epoch   2 |   256/  320 batches | accuracy    0.131 | perplexity  778.131\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 58.09s | valid accuracy    0.125 | valid perplexity  635.740 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Call the train function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "lr = 0.0005\n",
    "epochs = 3\n",
    "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, epochs, optimiser, loss_function, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8667411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, num_words, temperature=1.0):\n",
    "    '''\n",
    "    model.eval()\n",
    "    words = tokeniser(start_text)\n",
    "    hidden = model.init_hidden(1)\n",
    "    for i in range(0, num_words):\n",
    "        x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device)\n",
    "        y_pred, hidden = model(x, hidden)\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = (F.softmax(last_word_logits / temperature, dim=0).detach()).to(device='cpu').numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(vocab.lookup_token(word_index))\n",
    "\n",
    "    return ' '.join(words)\n",
    "    '''\n",
    "    \n",
    "    model.eval()\n",
    "    words = tokeniser(start_text)\n",
    "    hidden = model.init_hidden(1)\n",
    "    for i in range(0, num_words):\n",
    "        x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device)\n",
    "        y_pred, hidden = model(x, hidden)\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = (F.softmax(last_word_logits / temperature, dim=0).detach()).to(device='cpu').numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(vocab.lookup_token(word_index))\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some text\n",
    "print(generate_text(model, start_text=\"I like\", num_words=100, temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2884543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mexico ( spanish méxico ) , officially the united mexican states , is a of people for many <unk> and shifted gold for the <unk> of the <unk> , he who was money . building canaan oscar to the single , the churchyard saw the intersection as a track and the tuition , and it to get the right into support and inventory . , they ' s are captain , such as future <unk> , was expressed operating in the ninth river in the port , and the historic @-@ u education and that it said that they were a real what wished to love . it was played one of the episode\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"Mexico (Spanish: México), officially the United Mexican States, is a  \", num_words=100, temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.LSTM):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1cb126a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "# Feel free to experiment\n",
    "class LSTMModel2(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTMModel2, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0.5)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size, 500)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(500, vocab_size)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, text, hidden):\n",
    "        embeddings = self.embeddings(text)\n",
    "        output_lstm1, hidden = self.lstm(embeddings, hidden)\n",
    "        out_drop1 = self.dropout(output_lstm1)\n",
    "        out_fc1 = self.fc1(out_drop1)\n",
    "        decoded = self.fc2(out_fc1)\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = len(vocab) # vocabulary size\n",
    "emb_size = 300 # embedding size\n",
    "neurons = 256 # the dimension of the feedforward network model, i.e. # of neurons \n",
    "num_layers = 5 # the number of nn.LSTM layers\n",
    "model2 = LSTMModel2(vocab_size, emb_size, neurons, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    64/  320 batches | accuracy    0.062 | perplexity 1170.618\n",
      "| epoch   0 |   128/  320 batches | accuracy    0.097 | perplexity  929.465\n",
      "| epoch   0 |   192/  320 batches | accuracy    0.108 | perplexity  808.244\n",
      "| epoch   0 |   256/  320 batches | accuracy    0.120 | perplexity  724.486\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   0 | time: 111.23s | valid accuracy    0.141 | valid perplexity  492.439 \n",
      "-----------------------------------------------------------\n",
      "| epoch   1 |    64/  320 batches | accuracy    0.134 | perplexity  634.047\n",
      "| epoch   1 |   128/  320 batches | accuracy    0.139 | perplexity  522.719\n",
      "| epoch   1 |   192/  320 batches | accuracy    0.137 | perplexity  576.984\n",
      "| epoch   1 |   256/  320 batches | accuracy    0.139 | perplexity  568.515\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 116.27s | valid accuracy    0.163 | valid perplexity  375.683 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |    64/  320 batches | accuracy    0.161 | perplexity  453.719\n",
      "| epoch   2 |   128/  320 batches | accuracy    0.153 | perplexity  478.643\n",
      "| epoch   2 |   192/  320 batches | accuracy    0.155 | perplexity  435.433\n",
      "| epoch   2 |   256/  320 batches | accuracy    0.147 | perplexity  450.975\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 110.23s | valid accuracy    0.176 | valid perplexity  317.485 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "lr = 0.0005\n",
    "epochs = 3\n",
    "optimiser = optim.Adam(model2.parameters(), lr=lr)\n",
    "train(model2, epochs, optimiser, loss_function, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23424166666666665"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(147.2586, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(model2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de373d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this if cuda runs out of memory\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d82438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like music , and smoking that the whole hits of dylan ' s stage performance . the song has been released after their release , following the release . the same day is the original @-@ up website , calling the <unk> and the eye score in the game . in the episode , shot and a female , inspired that us @-@ owner through a 10th @-@ century @-@ legal @-@ pop mystery lesson , i can like more important stories . but i ' re going in everything to be originally used following , thus singing a label . after\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model2, start_text=\"I like music\", num_words=100, temperature=0.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what i like the most is destroyed , ( also infected\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model2, start_text=\"What I like the most is \", num_words=5, temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mexico ( spanish méxico ) , officially the united mexican states , is a indonesian movement , a series of special intellectual cities ,\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model2, start_text=\"Mexico (Spanish: México), officially the United Mexican States, is a  \", num_words=10, temperature=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "global_vectors = GloVe(name='6B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = WikiText2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = to_map_style_dataset(train_dataset), to_map_style_dataset(val_dataset), to_map_style_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "#set unknown token at position 0\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_len = 300\n",
    "seq_length = 50\n",
    "seq_lenght_extra = seq_length + 1\n",
    "def embed_vector_batch(batch):\n",
    "    X_embed = torch.zeros(len(batch), seq_length, embed_len)\n",
    "    Y = torch.zeros(len(batch), seq_length)\n",
    "\n",
    "    for i, batch_item in enumerate(batch):      \n",
    "        tokens = tokeniser(batch_item) \n",
    "        clean_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token == '': \n",
    "                continue\n",
    "            clean_tokens.append(token)\n",
    "\n",
    "        if (len(clean_tokens)) < seq_lenght_extra:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        vector = global_vectors.get_vecs_by_tokens(clean_tokens[:seq_lenght_extra])\n",
    "        if len(vector) >= seq_length:\n",
    "            X_embed[i] = vector[0:-1]\n",
    "            # Shift all tokens to make Y the next word prediction equivalent of X\n",
    "            Y[i] = torch.tensor(vocab(clean_tokens[1:seq_lenght_extra]), dtype=torch.long) \n",
    "\n",
    "    return X_embed, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # choose a batch size that fits your computation resources\n",
    "train_glove_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=embed_vector_batch)\n",
    "val_glove_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=embed_vector_batch)\n",
    "test_glove_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=embed_vector_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "# Feel free to experiment\n",
    "class LSTMModel_GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTMModel_GloVe, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        nn.init.kaiming_normal_(self.fc.weight)\n",
    "\n",
    "\n",
    "    def forward(self, X, hidden):\n",
    "        output, hidden = self.lstm(X, hidden)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = len(vocab) # vocabulary size\n",
    "emb_size = 300 # embedding size\n",
    "neurons = emb_size # the dimension of the feedforward network model, i.e. # of neurons \n",
    "num_layers = 2 # the number of nn.LSTM layers\n",
    "model_glove = LSTMModel_GloVe(vocab_size, emb_size, neurons, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_glove(model, loader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (data, targets) in enumerate(loader):\n",
    "            # Detach the hidden state between batches\n",
    "            hidden = model.init_hidden(batch_size) \n",
    "            \n",
    "            xi = data.to(device=device, dtype = torch.float32)\n",
    "            yi = targets.to(device=device, dtype = torch.long) \n",
    "            yi_flatten = yi.view(-1)\n",
    "\n",
    "            y_pred, hidden = model(xi, hidden)\n",
    "            y_pred_max_prob, y_pred_max_idx = y_pred.max(dim=2)\n",
    "\n",
    "            total_acc += (y_pred_max_idx.view(-1) == yi_flatten).sum().item()\n",
    "            total_count += yi_flatten.size(0)\n",
    "            \n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.text import Perplexity\n",
    "\n",
    "def perplexity_glove(model, loader):\n",
    "    model.eval()\n",
    "    perplexity_metric = Perplexity(device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, targets) in enumerate(loader):\n",
    "            # Detach the hidden state between batches\n",
    "            hidden = model.init_hidden(batch_size) \n",
    "            \n",
    "            xi = data.to(device=device, dtype = torch.float32)\n",
    "            yi = targets.to(device=device, dtype = torch.long) \n",
    "\n",
    "            y_pred, hidden = model(xi, hidden)\n",
    "\n",
    "            perplexity_metric.update(y_pred, yi)\n",
    "            \n",
    "    return perplexity_metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_glove(model, epochs, optimiser, loss_function, train_loader, val_loader):\n",
    "    '''\n",
    "    The following are possible instructions you may want to conside for this function.\n",
    "    This is only a guide and you may change add or remove whatever you consider appropriate\n",
    "    as long as you train your model correctly.\n",
    "        - loop through specified epochs\n",
    "        - loop through dataloader\n",
    "        - don't forget to zero grad!\n",
    "        - place data (both input and target) in device\n",
    "        - init hidden states e.g. hidden = model.init_hidden(batch_size)\n",
    "        - run the model\n",
    "        - compute the cost or loss\n",
    "        - backpropagation\n",
    "        - Update paratemers\n",
    "        - Include print all the information you consider helpful\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        total_acc, total_count = 0, 0\n",
    "        log_interval = int(len(train_loader) / 5)\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "\n",
    "        for i, (data, targets) in enumerate(train_loader):\n",
    "            # Detach the hidden state between batches\n",
    "            hidden = model.init_hidden(batch_size) \n",
    "        \n",
    "            xi = data.to(device=device, dtype = torch.float32)\n",
    "            yi = targets.to(device=device, dtype = torch.long)\n",
    "            yi_flatten = yi.view(-1)\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            y_pred, hidden = model(xi, hidden)\n",
    "            y_pred_max_prob, y_pred_max_idx = y_pred.max(dim=2)\n",
    "\n",
    "            # input needs to match the yi flattened size in one dimension and the vocab_size (our classes) on another\n",
    "            cost = loss_function(input=y_pred.view(batch_size*seq_length, vocab_size), target=yi_flatten)\n",
    "            cost.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimiser.step()\n",
    "\n",
    "            total_acc += (y_pred_max_idx.view(-1) == yi_flatten).sum().item()\n",
    "            total_count += yi_flatten.size(0)\n",
    "\n",
    "            perplexity_metric = Perplexity(device=device)\n",
    "            perplexity_metric.update(y_pred, yi)\n",
    "            perplex_test = perplexity_metric.compute()\n",
    "            \n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                print(\n",
    "                    \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                    \"| accuracy {:8.3f} | perplexity {:8.3f}\".format(\n",
    "                        epoch, i, len(train_loader), total_acc / total_count, perplex_test\n",
    "                    )\n",
    "                )\n",
    "            total_acc, total_count = 0, 0\n",
    "\n",
    "        accu_val = accuracy_glove(model, val_loader)\n",
    "        perplex_val = perplexity_glove(model, val_loader)\n",
    "\n",
    "        print(\"-\" * 59)\n",
    "        print(\n",
    "            \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "            \"valid accuracy {:8.3f} | valid perplexity {:8.3f} \".format(\n",
    "                epoch, time.time() - start_time, accu_val, perplex_val\n",
    "            )\n",
    "        )\n",
    "        print(\"-\" * 59)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    57/  286 batches | accuracy    0.710 | perplexity    7.947\n",
      "| epoch   0 |   114/  286 batches | accuracy    0.708 | perplexity    8.256\n",
      "| epoch   0 |   171/  286 batches | accuracy    0.755 | perplexity    5.804\n",
      "| epoch   0 |   228/  286 batches | accuracy    0.699 | perplexity    8.991\n",
      "| epoch   0 |   285/  286 batches | accuracy    0.757 | perplexity    5.840\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   0 | time: 86.57s | valid accuracy    0.695 | valid perplexity    8.402 \n",
      "-----------------------------------------------------------\n",
      "| epoch   1 |    57/  286 batches | accuracy    0.708 | perplexity    7.845\n",
      "| epoch   1 |   114/  286 batches | accuracy    0.656 | perplexity   11.528\n",
      "| epoch   1 |   171/  286 batches | accuracy    0.673 | perplexity   10.393\n",
      "| epoch   1 |   228/  286 batches | accuracy    0.747 | perplexity    5.865\n",
      "| epoch   1 |   285/  286 batches | accuracy    0.791 | perplexity    4.423\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 85.86s | valid accuracy    0.696 | valid perplexity    8.214 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |    57/  286 batches | accuracy    0.654 | perplexity   11.453\n",
      "| epoch   2 |   114/  286 batches | accuracy    0.691 | perplexity    8.583\n",
      "| epoch   2 |   171/  286 batches | accuracy    0.609 | perplexity   14.833\n",
      "| epoch   2 |   228/  286 batches | accuracy    0.734 | perplexity    6.684\n",
      "| epoch   2 |   285/  286 batches | accuracy    0.709 | perplexity    7.616\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 85.31s | valid accuracy    0.700 | valid perplexity    8.039 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "lr = 0.0005\n",
    "epochs = 3\n",
    "optimiser = optim.Adam(model_glove.parameters(), lr=lr)\n",
    "train_glove(model_glove, epochs, optimiser, loss_function, train_glove_loader, val_glove_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_glove, \"lstm_glove_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7042647058823529"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_glove(model_glove, test_glove_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.6612, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_glove(model_glove, test_glove_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_glove(model, start_text, num_words, temperature=1.0):\n",
    "    '''\n",
    "    model.eval()\n",
    "    words = tokeniser(start_text)\n",
    "    hidden = model.init_hidden(1)\n",
    "    for i in range(0, num_words):\n",
    "        x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device)\n",
    "        y_pred, hidden = model(x, hidden)\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = (F.softmax(last_word_logits / temperature, dim=0).detach()).to(device='cpu').numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(vocab.lookup_token(word_index))\n",
    "\n",
    "    return ' '.join(words)\n",
    "    '''\n",
    "    \n",
    "    model.eval()\n",
    "    words = tokeniser(start_text)\n",
    "\n",
    "    # while len(words) < 50:\n",
    "    #     words.append('')\n",
    "\n",
    "    hidden = model.init_hidden(1)\n",
    "    for i in range(0, num_words):\n",
    "        # x = torch.tensor(global_vectors.get_vecs_by_tokens(words), dtype=torch.float32, device=device)\n",
    "        x = global_vectors.get_vecs_by_tokens(words)\n",
    "        x = x.to(device=device, dtype=torch.float32)\n",
    "        y_pred, hidden = model(x.reshape([1, len(words), emb_size]), hidden)\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = (F.softmax(last_word_logits / temperature, dim=0).detach()).to(device='cpu').numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(vocab.lookup_token(word_index))\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like various critics . in the explained in britain , the oldest <unk> also also screened by the <unk> , as a major male and stem jackrabbit ( prince and 73 metres ) . it is discovered on the commercial road of khouw and entrance in the northern order . inscriptions praised that a small face of the <unk> ( long between the 8th ) has been established under the domestic bc . branch of the naval church , croatia , are delicate shaped @-@ , marina , <unk> and rape , which in the turning of caribbean areas area .\n"
     ]
    }
   ],
   "source": [
    "# Generate some text\n",
    "print(generate_text_glove(model_glove, start_text=\"I like\", num_words=100, temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like music ' s <unk> and the british ' s name books . in the world , musical guitar , lisa hero and robert <unk> , as these window , based to in each of his inventor applied except by president ' s car , but he towed police armor . it was statehood in london and 1924 in henry powers ( kings of the italian <unk> in america ) . a result of halmahera is took yet racial from the <unk> as one of the feast of <unk> ( <unk> ) in zhou . night , causing turkey , while in\n"
     ]
    }
   ],
   "source": [
    "print(generate_text_glove(model_glove, start_text=\"I like music\", num_words=100, temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mexico ( spanish méxico ) , officially the united mexican states , is a dog , hawaii <unk> in sagebrush . it was from\n"
     ]
    }
   ],
   "source": [
    "print(generate_text_glove(model_glove, start_text=\"Mexico (Spanish: México), officially the United Mexican States, is a  \", num_words=10, temperature=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
